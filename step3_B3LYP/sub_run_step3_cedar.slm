#!/bin/sh
##SBATCH --account=rrg-tzeng
#SBATCH --account=def-tzeng-ab
#SBATCH --time=0-05:59           # DD-HH:MM
#SBATCH --nodes=1
####SBATCH --tasks-per-node=4     # MPI tasks
####SBATCH --cpus-per-task=1
##SBATCH --mem-per-cpu=8G
##SBATCH --mem=0                 # all memory on node

##SBATCH --ntasks=32
##SBATCH --mem-per-cpu=4096

##SBATCH --constraint=CPU-Gold-6130
##SBATCH --requeue
#SBATCH --ntasks-per-node=40
#SBATCH --cpus-per-task=1
#SBATCH --mem=128000

echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST="$SLURM_JOB_NODELIST
echo "SLURM_NNODES="$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo "working directory="$SLURM_SUBMIT_DIR


module load StdEnv/2020
module load gcc python/3.8
#source ~/libra/bin/activate
#export PYTHONPATH=~/libra/libra-code/build/src

#source /home/ekadashi/libra/bin/activate
#module load StdEnv/2020 gcc/9.3.0
#module load hdf5/1.12.1

#module load cuda/6.5
#module load vmd
#module load openmpi/3.0.3/gcc-7.3.0
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

if [ -n "$SLURM_CPUS_PER_TASK" ]; then
  omp_threads=$SLURM_CPUS_PER_TASK
else
  omp_threads=1
fi
export OMP_NUM_THREADS=$omp_threads


# Here load whatever which is needed for running CP2K
#module load StdEnv/2020  intel/2020.1.217  openmpi/4.0.3
#module load cp2k/7.1

#module use /projects/academic/cyberwksp21/MODULES
#module load cp2k/v24/avx512
#module load libra/devel
#module load vmd

#python run.py
#module load hdf5/1.12.1
python run_step3.py
